# Перемножение матриц

Алгоритм перемножения матриц на GPU и CPU представлен и реализован на языке Python в блокноте.

Ниже представлена таблица, отражающая время выполнения в секундах операции умножения с применением технологии CUDA и на одном ядре процессора (CPU), а также ускорение:

| Размерность | CPU                | GPU                  | Ускорение |
| ----------- | ------------------ | -------------------- | --------- |
| 100x100     | 0.2834045886993408 | 0.011012554168701172 | 26        |
| 250x250     | 4.3014092445373535 | 0.011156320571899414 | 386       |
| 500x500     | 36.37232732772827  | 0.022998332977294922 | 1581      |
| 1000x1000   | 284.6461319923401  | 0.05729794502258301  | 2102      |
| 1500x1500   | 940.990317106247   | 0.3503744602203369   | 2686      |
| 2000x2000   | 2339.289197921753  | 0.21313738822937012  | 10975     |

Снижение времени и резкое увеличение ускорения на GPU можно характеризовать тем, что физически GPU может обрабатывать определённое количество потоков одновременно. с увеличением размерности матрицы создаётся такая сетка с такой размерностью блоков, что все нити выполняют приблизительно равное количество операций и это количество стремится к минимуму.

## Вывод:

Применение графического процессора (GPU) обеспечивает значительное увеличение скорости, и это ускорение продолжает расти при увеличении размера задачи.